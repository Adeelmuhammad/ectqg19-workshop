---
title: 'R: Data structures'
author: "Roger Bivand"
date: "Thursday, 5 September 2019, 09:45-10:15"
output:
  pdf_document: default
  html_document: null
link-citations: yes
bibliography: rmd.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

### Required current contributed CRAN packages:

I am running R 3.6.1, with recent `update.packages()`.

```{r, echo=TRUE}
needed <- c("sf", "stars", "lwgeom")
```

## Help, examples and built-in datasets in R

- In RStudio, the Help tab in the lower right pane (default position) gives access to the R manuals and to the installed packages help pages through the Packages link under Reference

- In R itself, help pages are available in HTML (browser) and text form; `help.start()` uses the default browser to display the Manuals, Reference and Miscellaneous Material sections in RStudio's home help tab

- The search engine can be used to locate help pages, but is not great if many packages are installed, as no indices are stored 

- The help system needs to be learned in order to provide the user with ways of progressing without wasting too much time

- The base help system does not tell you how to use R as a system, about packages not installed on your machine, or about R as a community

- It does provide information about functions, methods and (some) classes in base R and in contributed packages installed on your machine

- There are different requirements with regard to help systems --- in R, the help pages of base R are expected to be accurate although terse

### Help pages

- Each help page provides a short description of the functions, methods or classes it covers; some pages cover more than one such

- Help pages are grouped by package, so that the browser-based system is not easy to browse if you do not know which package a function belongs to

- The usage of the function is shown explicitly, including any defaults for arguments to functions or methods

- Each argument is described, showing names and types; in addition details of the description are given, together with the value returned

### Interactive use of help pages

- Rather than starting from the packages hierarchy of help pages, users most often use the `help()` function

- The function takes the name of of the function about which we need help, the name may be in quotation marks; class names contain a hyphen and must be quoted

- Instead of using say `help(help)`, we can shorten to the question mark operator: `?help`

- Occasionally, several packages offer different functions with the same name, and we may be offered a choice; we can disambiguate by putting the package name and two colons before the function name

### Function arguments

- In the usage section, function arguments are shown by name and order; the `args()` function returns information

- In general, if arguments are given by name, the order is arbitrary, but if names are not used at least sometimes, order matters

- Some arguments do not have default values and are probably required, although some are guessed if missing

- Being explicit about the names of arguments and the values they take is helpful in scripting and reproducible research

- The ellipsis `...` indicates that the function itself examines objects passed to see what to do

### Tooltips and completion

- The regular R console does not provide tooltips, that is a bubble first offering alternative function or object names as you type, then lists of argument names

- RStudio, like many IDEs, does provide this, controlled by Tools -> Global options -> Code -> Completion (by default it is operative)

- This may be helpful or not, depending on your style of working; if you find it helpful, fine, if not, you can make it less invasive under Global options

- Other IDE have also provided this facility, which builds directly on the usage sections of help pages of functions in installed packages

### Coherence code/documentation

- Base R has a set of checks and tests that ensure coherence between the code itself and the usage sections in help pages

- These mechanisms are used in checking contributed packages before they are released through the the archive network; the description of arguments on help pages must match the function definition

- It is also possible to generate help pages documenting functions automatically, for example using the **roxygen2** package

- It is important to know that we can rely on this coherence


### Returned values

- The objects returned by functions are also documented on help pages, but the coherence of the description with reality is harder to check

- This means that use of `str()` or other functions or methods may be helpful when we want to look inside the returned object

- The form taken by returned values will often also vary, depending on the arguments given

- Most help pages address this issue not by writing more about the returned values, but by using the examples section to highlight points of potential importance for the user

### Examples

- Reading the examples section on the help page is often enlightening, but we do not need to copy and paste

- The `example()` function runs those parts of the code in the examples section of a function that are not tagged don't run --- this can be overridden, but may involve meeting conditions not met on your machine

- This code is run nightly on CRAN servers on multiple operating systems and using released, patched and development versions of R, so checking both packages and the three versions of R

- Some examples use data given verbatim, but many use built-in data sets; most packages also provide data sets to use for running examples

### Built-in data sets

- This means that the examples and the built-in data sets are a most significant resource for learning how to solve problems with R

- Very often, one recognizes classic textbook data sets from the history of applied statistics; contemporary text book authors often publish collections of data sets as packages on CRAN

- The built-in data sets also have help pages, describing their representation as R objects, and their licence and copyright status

- These help pages also often include an examples section showing some of the analyses that may be carried out using them

- One approach that typically works well when you have a data set of your own, but are unsure how to proceed, is to find a built-in data set that resembles the real one, and play with that first

- The built-in data sets are often quite small, and if linked to text books, they are well described there as well as in the help pages

- By definition, the built-in data sets do not have to be imported into R, as they are almost always stored as files of R objects

- In some cases, these data sets are stored in external file formats, most often to show how to read those formats

- The built-in data sets in the base **datasets** package are in the search path, but data sets in other packages should be loaded using the `data()` function


## Vectors, matrices and `data.frames`

### Simple vectors

In R, scalars are vectors of unit length, so most data are vectors or combinations of vectors. The printed results are prepended by a curious `[1]`; all these results are unit length vectors. We can combine several objects with `c()`:

```{r vect1, echo = TRUE}
a <- c(2, 3)
a
sum(a)
str(a)
aa <- rep(a, 50)
aa
```

The single square brackets `[]` are used to access or set elements of vectors (the colon `:` gives an integer sequence); negative indices drop elements:

```{r vect2, echo = TRUE}
length(aa)
aa[1:10]
sum(aa)
sum(aa[1:10])
sum(aa[-(11:length(aa))])
```

### Arithmetic under the hood

Infix syntax is just a representation of the actual underlying forms

```{r vect2a, echo = TRUE}
a[1] + a[2]
sum(a)
`+`(a[1], a[2])
Reduce(`+`, a)
```

We've done arithmetic on scalars, we can do vector-scalar arithmetic:

```{r vect3, echo = TRUE}
sum(aa)
sum(aa+2)
sum(aa)+2
sum(aa*2)
sum(aa)*2
```

But vector-vector arithmetic poses the question of vector length and recycling (the shorter one gets recycled):


```{r vect4, echo = TRUE}
v5 <- 1:5
v2 <- c(5, 10)
v5 * v2
v2_stretch <- rep(v2, length.out=length(v5))
v2_stretch
v5 * v2_stretch
```

In working with real data, we often meet missing values, coded by NA meaning Not Available:

```{r NA, echo = TRUE}
anyNA(aa)
is.na(aa) <- 5
aa[1:10]
anyNA(aa)
sum(aa)
sum(aa, na.rm=TRUE)
```

### Checking data


One way to check our input data is to print in the console --- this works with small objects as we've seen, but for larger objects we need methods:


```{r check1, echo = TRUE}
big <- 1:(10^5)
length(big)
head(big)
str(big)
summary(big)
```

### Basic vector types

There are `length`, `head`, `str` (*str*ucture) and `summary` methods for many types of objects. `str` also gives us a hint of the type of object and its dimensions. We've seen a couple of uses of `str` so far, `str(a)` was `num` and `str(big)` was `int`, what does this signify? They are both numbers, but of different types.

There are six basic vector types: list, integer, double, logical, character and [complex](http://www.johnmyleswhite.com/notebook/2009/12/18/using-complex-numbers-in-r/). The derived type factor (to which we return shortly) is integer with extra information. `str` reports these as int, num, logi, chr and cplx, and lists are enumerated recursively. In RStudio you see more or less the `str` output in the environment pane as Values in the list view; the grid view adds the object size in memory. From early S, we have `typeof` and `storage.mode` (including single precision, not used in R) --- these are important for interfacing C, C++, Fortran and other languages. Beyond this is `class`, but then the different class systems (S3 and formal S4) complicate things. Objects such as vectors may also have attributes in which their class and other information may be placed. Typically, a lot of use is made of attributes to squirrel away strings and short vectors. 

`is` methods are used to test types of objects; note that integers are also seen as numeric:

```{r coerce1, echo = TRUE}
set.seed(1)
x <- runif(50, 1, 10)
is.numeric(x)
y <- rpois(50, lambda=6)
is.numeric(y)
is.integer(y)
xy <- x < y
is.logical(xy)
```

`as` methods try to convert between object types and are widely used:

```{r coerce2, echo = TRUE}
str(as.integer(xy))
str(as.numeric(y))
str(as.character(y))
str(as.integer(x))
```

### The data frame object

First, let us see that is behind the `data.frame` object: the `list` object. `list` objects are vectors that contain other objects, which can be addressed by name or by 1-based indices. Like the vectors we have already met, lists can be  accessed and manipulated using square brackets `[]`. Single list elements can be accessed and manipulated using double square brackets `[[]]`. 

### List objects

Starting with four vectors of differing types, we can assemble a list object; as we see, its structure is quite simple. The vectors in the list may vary in length, and lists can (and do often) include lists

```{r , echo = TRUE}
V1 <- 1:3
V2 <- letters[1:3]
V3 <- sqrt(V1)
V4 <- sqrt(as.complex(-V1))
L <- list(v1=V1, v2=V2, v3=V3, v4=V4)
str(L)
L$v3[2]
L[[3]][2]
```

### Data Frames

Our `list` object contains four vectors of different types but of the same length; conversion to a `data.frame` is convenient. Note that by default strings are converted into factors:

```{r , echo = TRUE}
DF <- as.data.frame(L)
str(DF)
DF <- as.data.frame(L, stringsAsFactors=FALSE)
str(DF)
```

We can also provoke an error in conversion from a valid `list` made up of vectors of different length to a `data.frame`:


```{r , echo = TRUE}
V2a <- letters[1:4]
V4a <- factor(V2a)
La <- list(v1=V1, v2=V2a, v3=V3, v4=V4a)
DFa <- try(as.data.frame(La, stringsAsFactors=FALSE), silent=TRUE)
message(DFa)
```

We can access `data.frame` elements as `list` elements, where the `$` is effectively the same as `[[]]` with the list component name as a string:

```{r , echo = TRUE}
DF$v3[2]
DF[[3]][2]
DF[["v3"]][2]
```

Since a `data.frame` is a rectangular object with named columns with equal numbers of rows, it can also be indexed like a matrix, where the rows are the first index and the columns (variables) the second:


```{r , echo = TRUE}
DF[2, 3]
DF[2, "v3"]
str(DF[2, 3])
str(DF[2, 3, drop=FALSE])
```

If we coerce a `data.frame` containing a character vector or factor into a matrix, we get a character matrix; if we extract an integer and a numeric column, we get a numeric matrix.

```{r , echo = TRUE}
as.matrix(DF)
as.matrix(DF[,c(1,3)])
```

The fact that `data.frame` objects descend from `list` objects is shown by looking at their lengths; the length of a matrix is not its number of columns, but its element count:

```{r , echo = TRUE}
length(L)
length(DF)
length(as.matrix(DF))
```

There are `dim` methods for `data.frame` objects and matrices (and arrays with more than two dimensions); matrices and arrays are seen as vectors with dimensions; `list` objects have no dimensions:


```{r , echo = TRUE}
dim(L)
dim(DF)
dim(as.matrix(DF))
```

```{r , echo = TRUE}
str(as.matrix(DF))
```

`data.frame` objects have `names` and `row.names`, matrices have `dimnames`, `colnames` and `rownames`; all can be used for setting new values:

```{r , echo = TRUE}
row.names(DF)
names(DF)
names(DF) <- LETTERS[1:4]
names(DF)
str(dimnames(as.matrix(DF)))
```

R objects have attributes that are not normally displayed, but which show their structure and class (if any); we can see that `data.frame` objects are quite different internally from matrices:


```{r , echo = TRUE}
str(attributes(DF))
str(attributes(as.matrix(DF)))
```

If the reason for different vector lengths was that one or more observations are missing on that variable, `NA` should be used; the lengths are then equal, and a rectangular table can be created:

```{r , echo = TRUE}
V1a <- c(V1, NA)
V3a <- sqrt(V1a)
La <- list(v1=V1a, v2=V2a, v3=V3a, v4=V4a)
DFa <- as.data.frame(La, stringsAsFactors=FALSE)
str(DFa)
```


## New style spatial vector representation

### The **sf** package

The recent **sf** package bundles GDAL and GEOS (**sp** just defined the classes and methods, leaving I/O and computational geometry to other packages **rgdal** and **rgeos**). **sf** uses `data.frame` objects with one (or more) geometry column for vector data. The representation follows ISO 19125 (*Simple Features*), and has WKT (text) and WKB (binary) representations (used by GDAL and GEOS internally). The drivers include PostGIS and other database constructions permitting selection, and WFS for server APIs. These are the key references for **sf**: [@geocompr], [@sdsr], [@RJ-2018-009], package [vignettes](https://cran.r-project.org/package=sf) and blog posts on (https://www.r-spatial.org/).


```{r, echo=TRUE}
library(sf)
```

The `st_read()` method, here for a `"character"` first object giving the file name and path, uses GDAL through **Rcpp** to identify the driver required, and to use it to read the feature geometries and fields. The character string fields are not converted to `"factor"` representation, as they are not categorical variables:

```{r, echo=TRUE}
lux <- st_read("../data/lux_regions.gpkg", stringsAsFactors=FALSE)
```

The vector drivers available to me with my GDAL build are:

```{r, echo=TRUE}
st_drivers(what="vector")[,c(2:4, 7)]
```

Package **sf** provides handling of feature data, where feature
geometries are points, lines, polygons or combinations of those.
It implements the full set of geometric functions described in the
_simple feature access_ standard, and some. The basic storage is
very simple, and uses only base R types (list, matrix).

* feature sets are held as records (rows) in `"sf"` objects, inheriting from `"data.frame"`
* `"sf"` objects have at least one simple feature geometry list-column of class `"sfc"`
* geometry list-columns are *sticky*, that is they stay stuck to the object when subsetting columns, for example using `[`
* `"sfc"` geometry list-columns have a bounding box and a coordinate reference system as attribute, and a class attribute pointing out the common type (or `"GEOMETRY"` in case of a mix)
* a single simple feature geometry is of class `"sfg"`, and further classes pointing out dimension and type

Storage of simple feature geometry:

* `"POINT"` is a numeric vector
* `"LINESTRING"` and `"MULTIPOINT"` are numeric matrix, points/vertices in rows
* `"POLYGON"` and `"MULTILINESTRING"` are lists of matrices
* `"MULTIPOLYGON"` is a lists of those
* `"GEOMETRYCOLLECTION"` is a list of typed geometries


```{r, echo=TRUE}
class(lux)
```

The columns of the `"data.frame"` object have these names:

```{r, echo=TRUE}
names(lux)
```

Two of the attributes of the object are those all `"data.frame"` objects possess: `names` shown above and `row.names`. The fourth, `sf_column` gives the name of the active geometry column.

```{r, echo=TRUE}
names(attributes(lux))
```

The `$` access operator lets us operate on a single column of the object as with any other `"data.frame"` object:

```{r, echo=TRUE}
hist(lux$ghsl_pop)
```

Using the attribute value to extract the name of the geometry column, and the `[[` access operator to give programmatic access to a column by name, we can see that the `"sfc"` object is composed of `POLYGON` objects:

```{r, echo=TRUE}
class(lux[[attr(lux, "sf_column")]])
```

The geometry column is a list column, of the same length as the other columns in the `"data.frame"` object.

```{r, echo=TRUE}
is.list(lux[[attr(lux, "sf_column")]])
```

`"sf"` objects may be subsetted by row and column in the same way as regular `"data.frame"` objects, with the implicit understanding that the geometry column is _sticky_; here we choose only the first column, but the geometry column follows along, _stuck_ to the subsetted object, and obviously subsetted by row too.


```{r, echo=TRUE}
class(lux[1:5, 1])
```

Geometry columns have their own list of attributes, the count of empty geometries, the coordinate reference system, the precision and the bounding box (subsetting will refresh the bounding box; transformation will update the coordinate reference system and the bounding box):

```{r, echo=TRUE}
attributes(lux[[attr(lux, "sf_column")]])
```
The coordinate reference system is an object of class `"crs"`:

```{r, echo=TRUE}
class(attr(lux[[attr(lux, "sf_column")]], "crs"))
```

It contains an integer EPSG code (so far not compound codes), and a PROJ string:

```{r, echo=TRUE}
str(attr(lux[[attr(lux, "sf_column")]], "crs"))
```

Objects of this class can be instantiated for example by giving the relevant EPSG code:

```{r, echo=TRUE}
st_crs(4674)
```

```{r, echo=TRUE}
st_crs(31983)
```

We can drill down to the first feature geometry `"sfg"` object, which is a matrix with a class attribute - a vector of three elements, `"XY"` for two dimensions, `"POLYGON"` for the simple features definition, and `"sfg"` as the container class:

```{r, echo=TRUE}
str(lux[[attr(lux, "sf_column")]][[1]])
```

### Checking the data

Stepping back, we can try to check where the `POPULATION` field/column came from; the Luxembourg public data source provides a file, here as `"geojson"` https://data.public.lu/en/datasets/population-per-municipality/:

```{r, echo=TRUE}
pop <- st_read("../data/statec_population_by_municipality.geojson")
```

The difference is in the ordering of the features in the two objects:

```{r, echo=TRUE}
all.equal(pop$POPULATION, lux$POPULATION)
o <- match(as.character(pop$LAU2), as.character(lux$LAU2))
all.equal(pop$POPULATION, lux$POPULATION[o])
```

We'll also be able to look at the differences between population counts from this data source and from GHSL https://ghsl.jrc.ec.europa.eu/; the data source is probably the 2011 census:

```{r, echo=TRUE}
plot(lux[, c("POPULATION", "ghsl_pop")])
```

The trees are from https://data.public.lu/en/datasets/remarkable-trees/, but in projected, not geographical coordinates:

```{r, echo=TRUE}
trees <- st_read("../data/trees/anf_remarkable_trees_0.shp")
```

If we would like to find the areas of the administrative units, we could use spherical areas from **lwgeom**, or take them by transforming to for example the projection of the trees:

```{r, echo=TRUE}
area_sph <- lwgeom::st_geod_area(lux)
lux_tmerc <- st_transform(lux, 2169)
area_tmerc <- st_area(lux_tmerc)
```

There are small differences between these area outputs:

```{r, echo=TRUE}
lux_tmerc$area <- area_tmerc
lux_tmerc$area_err <- (lux_tmerc$area - area_sph)/lux_tmerc$area
summary(lux_tmerc$area_err)
```


```{r, echo=TRUE}
plot(lux_tmerc[, "area_err"], axes=TRUE, main="area difference in m2 per m2")
```

The area is in square meters, so we can use facilities in **units** to change to square kilometers to calculate population densities:

```{r, echo=TRUE}
units(lux_tmerc$area)
```

```{r, echo=TRUE}
units(lux_tmerc$area) <- "km^2"
units(lux_tmerc$area)
```

```{r, echo=TRUE}
lux_tmerc$pop_den <- lux_tmerc$POPULATION/lux_tmerc$area
summary(lux_tmerc$pop_den)
```

```{r, echo=TRUE}
lux_tmerc$ghsl_den <- lux_tmerc$ghsl_pop/lux_tmerc$area
summary(lux_tmerc$ghsl_den)
```

We can do the same kinds of sourcing checks with the tree counts:

```{r, echo=TRUE}
trees_sgbp <- st_intersects(lux_tmerc, trees)
trees_cnt <- sapply(trees_sgbp, length)
all.equal(trees_cnt, lux_tmerc$tree_count)
```

But we are not ready to move on yet.

### PROJ

Because so much open source (and other) software uses the PROJ library and framework, many are affected when PROJ upgrades. Until very recently, PROJ has been seen as very reliable, and the changes taking place now are intended to confirm and reinforce this reliability. Before PROJ 5 (PROJ 6 is out now, PROJ 7 is coming early in 2020), the `+datum=` tag was used, perhaps with `+towgs84=` with three or seven coefficients, and possibly `+nadgrids=` where datum transformation grids were available. However, transformations from one projection to another first inversed to longitude-latitude in WGS84, then projected on to the target projection.


### Big bump coming:

'Fast-forward 35 years and PROJ.4 is everywhere: It provides coordinate handling for almost every geospatial program, open or closed source. Today,  we  see  a  drastical  increase  in  the  need  for  high  accuracy  GNSS  coordinate  handling, especially in the agricultural and construction engineering sectors.  This need for geodetic-accuracy transformations  is  not  satisfied  by  "classic  PROJ.4".  But  with  the  ubiquity  of  PROJ.4,  we  can provide these transformations "everywhere", just by implementing them as part of PROJ.4' [@evers+knudsen17].


### Escaping the WGS84 hub/pivot: PROJ and OGC WKT2


Following the introduction of geodetic modules and pipelines in PROJ 5 [@knudsen+evers17; @evers+knudsen17], PROJ 6 moves further. Changes in the legacy PROJ representation and WGS84 transformation hub have been coordinated through the [GDAL barn raising](https://gdalbarn.com/) initiative. Crucially WGS84 often ceases to be the pivot for moving between datums. A new OGC WKT is coming, and an SQLite EPSG file database has replaced CSV files. SRS will begin to support 3D by default, adding time too as SRS change. See also [PROJ migration notes](https://proj.org/development/migration.html).

There are very useful postings on the PROJ mailing list from Martin Desruisseaux, first [proposing clarifications](https://lists.osgeo.org/pipermail/proj/2019-July/008748.html) and a [follow-up](https://lists.osgeo.org/pipermail/proj/2019-August/008750.html) including a summary:

> * "Early binding": hub transformation technique.

> * "Late binding": hub transformation technique NOT used, replaced by
a more complex technique consisting in searching parameters in the
EPSG database after the transformation context (source, target,
epoch, area of interest) is known.

> * The problem of hub transformation technique is independent of WGS84.
It is caused by the fact that transformations to/from the hub are
approximate. Any other hub we could invent in replacement of WGS84
will have the same problem, unless we can invent a hub for which
transformations are exact (I think that if such hub existed, we
would have already heard about it).

> The solution proposed by ISO 19111 (in my understanding) is:

> * Forget about hub (WGS84 or other), unless the simplicity of
early-binding is considered more important than accuracy.

> * Associating a CRS to a coordinate set (geometry or raster) is no
longer sufficient. A {CRS, epoch} tuple must be associated. ISO
19111 calls this tuple "Coordinate metadata". From a programmatic
API point of view, this means that getCoordinateReferenceSystem()
method in Geometry objects (for instance) needs to be replaced by a
getCoordinateMetadata() method.

Maybe watch Even Roualt's recent FOSS4G talk: https://media.ccc.de/v/bucharest-198-revamp-of-coordinate-reference-system-management-in-the-osgeo-c-c-stack-with-proj-and-gdal

The `projinfo` utility is available with the external PROJ library, so most likely not in general; this is for PROJ 6.1.1 (current as of writing):

```{r, echo=TRUE}
cat(system("projinfo EPSG:4326 -o PROJ", intern=TRUE), sep="\n")
```



```{r, echo=TRUE}
cat(system("projinfo EPSG:4326 -o WKT1_GDAL", intern=TRUE), sep="\n")
```



```{r, echo=TRUE}
cat(system("projinfo EPSG:4326 -o WKT2_2018", intern=TRUE), sep="\n")
```



```{r, echo=TRUE}
cat(system("projinfo EPSG:2169 -o PROJ", intern=TRUE), sep="\n")
```




```{r, echo=TRUE}
cat(system("projinfo EPSG:2169 -o WKT1_GDAL", intern=TRUE), sep="\n")
```



```{r, echo=TRUE}
cat(system("projinfo EPSG:2169 -o WKT2_2018", intern=TRUE), sep="\n")
```



```{r, echo=TRUE}
cat(system("projinfo -s EPSG:4326 -t EPSG:2169 -o PROJ", intern=TRUE), sep="\n")
```

### Raster

The GHSL data from https://ghsl.jrc.ec.europa.eu/ and provided in the `ghsl_pop` column is not documented here (yet). Using the GeoTIFF file (which turns out to be `+proj=moll`), we can read using a GDAL driver, first into memory, then proxy:

```{r, echo=TRUE}
library(stars)
system.time(ghsl0 <- read_stars("../data/ghsl.tiff", proxy=FALSE))
ghsl0
```



```{r, echo=TRUE}
system.time(ghsl1 <- read_stars("../data/ghsl.tiff", proxy=TRUE))
ghsl1
```

```{r, echo=TRUE}
plot(ghsl0)
```

Using the aggregate method on the input raster in memory warped to the Luxembourg transverse Mercator projection, by the municipality boundaries in `lux_tmerc`, we can recover the population counts. 

```{r, echo=TRUE}
system.time(ghsl_sum0 <- aggregate(st_warp(ghsl0, crs=2169, cellsize=250, use_gdal=FALSE), lux_tmerc, sum))
```

Using the proxy in this case takes about the same time:

```{r, echo=TRUE}
system.time(ghsl_sum1 <- aggregate(st_warp(ghsl1, crs=2169, cellsize=250, use_gdal=FALSE), lux_tmerc, sum))
```

```{r, echo=TRUE}
system.time(ghsl_sum2 <- aggregate(ghsl0, st_transform(lux, crs=st_crs(ghsl0)$proj4string), sum))
```

The output values following warping are closely aligned with, but differ a little from those included in the vector object read to begin with; it looks as though the vector object was transformed to match the raster before aggregation:

```{r, echo=TRUE}
summary(cbind(orig=lux_tmerc$ghsl_pop, warp=ghsl_sum0$ghsl.tiff, warp_proxy=ghsl_sum1$ghsl.tiff, moll=ghsl_sum2$ghsl.tiff))
```

```{r, echo=TRUE}
lux_tmerc$ghsl_tiff <- ghsl_sum0$ghsl.tiff
lux_tmerc$ghsl_warp_diff <- lux_tmerc$ghsl_tiff - lux_tmerc$ghsl_pop
plot(lux_tmerc[,"ghsl_warp_diff"])
```

```{r, echo=TRUE}
st_write(lux_tmerc, "../data/lux_tmerc.gpkg", delete_dsn=TRUE)
```
